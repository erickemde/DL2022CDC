{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Deep Learning Models with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 5\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet(num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.1508\n",
      "Epoch [1/5], Step [200/600], Loss: 0.1135\n",
      "Epoch [1/5], Step [300/600], Loss: 0.0567\n",
      "Epoch [1/5], Step [400/600], Loss: 0.0188\n",
      "Epoch [1/5], Step [500/600], Loss: 0.0433\n",
      "Epoch [1/5], Step [600/600], Loss: 0.0889\n",
      "Epoch [2/5], Step [100/600], Loss: 0.0966\n",
      "Epoch [2/5], Step [200/600], Loss: 0.0476\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0226\n",
      "Epoch [2/5], Step [400/600], Loss: 0.0321\n",
      "Epoch [2/5], Step [500/600], Loss: 0.0312\n",
      "Epoch [2/5], Step [600/600], Loss: 0.1012\n",
      "Epoch [3/5], Step [100/600], Loss: 0.1151\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0109\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0289\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0274\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0323\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0150\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0507\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0557\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0106\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0302\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0160\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0078\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0184\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0233\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0055\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0773\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0409\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0381\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 99.01 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "# torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "one_batch = next(iter(test_loader))\n",
    "k = 3\n",
    "one_image = one_batch[0][k]\n",
    "one_label = one_batch[1][k]\n",
    "\n",
    "\n",
    "one_image_npy = one_image.squeeze().numpy()\n",
    "plt.imshow(one_image_npy, cmap='gray')\n",
    "\n",
    "image = one_image.reshape(1,28*28).to(device)\n",
    "label = one_label.to(device)\n",
    "print(one_label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
